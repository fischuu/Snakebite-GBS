# Alignment

## Preliminary Mock Reference
### Mapping stats
 
Import the mapping statistics to the Mock reference

```{r import flagstats}
flagstatFiles <- list.files(file.path(projFolder,"FASTQ", "TRIMMED", "alignments"), pattern="*.flagstat")
flagstats <- list()
for(i in 1:length(flagstatFiles)){
  flagstats[[i]] <- readLines(file.path(projFolder,"FASTQ", "TRIMMED", "alignments",flagstatFiles[i]))
}
```

Visualization of the alignments, red stars indicate the mock reference samples

```{r vis mapping stats}
par(oma=c(6,3,0,0))
mapStats <- matrix(0,ncol=length(flagstatFiles), nrow=2)

sampleNames <- gsub(".sam.flagstat", "", flagstatFiles)

colnames(mapStats) <- sampleNames

tmp <- as.numeric(sapply(strsplit(sapply(flagstats,"[",1), " +"),"[",1))
mapStats[1,] <- as.numeric(sapply(strsplit(sapply(flagstats,"[",5), " +"),"[",1))
mapStats[2,] <- tmp - mapStats[1,]

p <- barplot(mapStats, col=c(report.colours[1], report.colours[2]), las=2)

legend("topleft", pch=c(20,20), col=c(report.colours[2], report.colours[1]), legend=c("Unmapped", "Mapped"), fill="white")

# highlight the mock reference samples
mockFiles <- paste(mockSamples, ".sam.flagstat", sep="")
mockPos <- which(is.element(flagstatFiles, mockFiles))

points(p[mockPos], rep(200000, length(mockPos)), pch="*", col="red", cex=4)
```

```{r mapping percentage}
barplot(mapStats[1,] / (apply(mapStats,2,sum)) * 100, ylim=c(0,100), ylab="Mapping in Percent", col=report.colours[1], las=2)
```

And the same information in table format.

```{r}
mappingRates <- mapStats[1,] / apply(mapStats,2,sum)
out <- as.matrix(mappingRates)
colnames(out)[1] <- "Mapping rate"
DT::datatable(out)
```

The average mapping rate is `r mean(mappingRates)`.

### Coverage

Data mapped against the clusters and then the reads per cluster visualized
```{r importClusterCoverage, warning=FALSE}
clusterFiles <- list.files(file.path(projFolder, "FASTQ", "TRIMMED", "alignments_clusters"), pattern="*.coverage")
clusterCoverage <- read.table(file.path(projFolder, "FASTQ", "TRIMMED", "alignments_clusters", clusterFiles[1]))
names(clusterCoverage)[1:2] <- c("cluster", clusterFiles[1])

for(i in 2:length(clusterFiles)){
  tmp <- read.table(file.path(projFolder, "FASTQ", "TRIMMED", "alignments_clusters", clusterFiles[i]))
  names(tmp)[1:2] <- c("cluster", clusterFiles[i])
  clusterCoverage <- merge(clusterCoverage, tmp, by="cluster")
}
names(clusterCoverage)[2:(length(clusterFiles)+1)] <- clusterFiles
clusterCoverage[,1] <- as.numeric(gsub("Cluster", "", clusterCoverage[,1]))
clusterCoverage <- clusterCoverage[order(clusterCoverage[,1]),]
clusterCoverage <- clusterCoverage[is.na(clusterCoverage[,1])==FALSE,]

clusterCoverage.std <- t(t(clusterCoverage)/apply(clusterCoverage,2,sum))*100000
```

### Lorenz curve
Concentration measure of reads against clusters

```{r lorenz}
plot(cumsum(sort(clusterCoverage[,2] / sum(clusterCoverage[,2]))), type="l", xlab="Cluster", ylab="Concentration")
for(i in 3:ncol(clusterCoverage)){
  lines(cumsum(sort(clusterCoverage[,i] / sum(clusterCoverage[,i]))))
}
```

### Stats on coverage

These are the amounts of clusters with different samples. 

```{r cluster coverage stats}
coveredClusters <- apply(clusterCoverage[,-1]>0,1,sum)
barplot(table(coveredClusters), xlab="No. of different samples aligned to cluster", col=report.colours[1])
```

Now we have here the number of reads per coverage class. That means, instead of having it binary as in the previous plot, we now count all the reads per coverage group.

```{r reads per coverage group}
readsPerCoverageGroup <- c()

for(i in 1:max(coveredClusters)){
  if(sum(coveredClusters==i)>0){
    readsPerCoverageGroup[i] <-  sum(clusterCoverage[coveredClusters==i,-1])
  } else {
    readsPerCoverageGroup[i] <- 0
  }
}

names(readsPerCoverageGroup) <- 1:max(coveredClusters)

barplot(readsPerCoverageGroup, xlab="Coverage group", ylab="Reads on cluster group", col=report.colours[1])
```


```{r reads per coverage group in percent}
barplot(readsPerCoverageGroup/sum(readsPerCoverageGroup)*100, xlab="Coverage group", ylab="Reads on cluster group (in %)", ylim=c(0,100), col=report.colours[1])
```

Then still the number of clusters without coverage per sample

```{r clusters without coverage}
nonHittedClusters <- apply(clusterCoverage[,-1]==0,2,sum)
names(nonHittedClusters) <- gsub(".coverage", "", names(nonHittedClusters))
barplot(nonHittedClusters, col=report.colours[1], las=2)
lines(c(0,100000), c(nrow(clusterCoverage), nrow(clusterCoverage)), lty="dotted")
```

And then this still as percentage

```{r clusterhits percentage}
barplot(nonHittedClusters/nrow(clusterCoverage), ylim=c(0,1), las=2, col=report.colours[1])
lines(c(0,10000),c(0.2,0.2), lty="dotted")
lines(c(0,10000),c(0.4,0.4), lty="dotted")
lines(c(0,10000),c(0.5,0.5), lty="dashed")
lines(c(0,10000),c(0.6,0.6), lty="dotted")
lines(c(0,10000),c(0.8,0.8), lty="dotted")
```


### Smoothed log-coverage per cluster

```{r vizCusterCoverage}
plot(smooth.spline(clusterCoverage[,1], log(clusterCoverage[,2]+1), all.knots=FALSE), type="l", xlab="Cluster", ylab="Log-coverage", ylim=c(0, max(log(clusterCoverage))/2))

for(i in 3:ncol(clusterCoverage)){
  lines(smooth.spline(clusterCoverage[,1], log(clusterCoverage[,i]+1), all.knots=FALSE))  
}
```

### Smoothed std-log-coverage per cluster

Now the coverages are divided by the total amount of reads per sample and then multiplied by 10^5.

```{r vizStdCusterCoverage}
# I use here the first column of the other matrix to keep the same coordinates. It does not affect the plot.
plot(smooth.spline(clusterCoverage[,1], log(clusterCoverage.std[,2]+1), all.knots=FALSE), type="l", xlab="Cluster", ylab="", ylim=c(0, max(log(clusterCoverage.std))/4))

for(i in 3:ncol(clusterCoverage)){
  lines(smooth.spline(clusterCoverage[,1], log(clusterCoverage.std[,i]+1), all.knots=FALSE))  
}
```
### Cluster coverage Mock samples vs others
ADD HERE STILL A COMPARISON, HOW THE READS FROM THE MOCK DISTRIBUTE ACROSS THE REFERENCE VS ALL OTHER SAMPLES

## Mock Reference
### Mapping stats

Import the mapping statistics to the Mock reference

```{r import final flagstats}
flagstatFiles <- list.files(file.path(projFolder,"BAM", "alignments_finalMock"), pattern="*.flagstat")
flagstats <- list()
for(i in 1:length(flagstatFiles)){
  flagstats[[i]] <- readLines(file.path(projFolder,"BAM", "alignments_finalMock",flagstatFiles[i]))
}
```

Visualization of the alignments, red stars indicate the mock reference samples

```{r vis mapping final stats}
par(oma=c(6,3,0,0))
mapStats <- matrix(0,ncol=length(flagstatFiles), nrow=2)

sampleNames <- gsub(".sam.flagstat", "", flagstatFiles)

colnames(mapStats) <- sampleNames

tmp <- as.numeric(sapply(strsplit(sapply(flagstats,"[",1), " +"),"[",1))
mapStats[1,] <- as.numeric(sapply(strsplit(sapply(flagstats,"[",5), " +"),"[",1))
mapStats[2,] <- tmp - mapStats[1,]

p <- barplot(mapStats, col=c(report.colours[1], report.colours[2]), las=2)

legend("topleft", pch=c(20,20), col=c(report.colours[2], report.colours[1]), legend=c("Unmapped", "Mapped"), fill="white")

# highlight the mock reference samples
mockFiles <- paste(mockSamples, ".sam.flagstat", sep="")
mockPos <- which(is.element(flagstatFiles, mockFiles))

points(p[mockPos], rep(200000, length(mockPos)), pch="*", col="red", cex=4)
```

```{r mapping final percentage}
barplot(mapStats[1,] / (apply(mapStats,2,sum)) * 100, ylim=c(0,100), ylab="Mapping in Percent", col=report.colours[1], las=2)
```

And the same information in table format.

```{r}
mappingRates <- mapStats[1,] / apply(mapStats,2,sum)
out <- as.matrix(mappingRates)
colnames(out)[1] <- "Mapping rate"
DT::datatable(out)
```

The average mapping rate is `r mean(mappingRates)`.

### Coverage

Data mapped against the clusters and then the reads per cluster visualized
```{r importFinalClusterCoverage, warning=FALSE}
clusterFiles <- list.files(file.path(projFolder, "BAM", "alignments_finalMock"), pattern="*.coverage")
clusterCoverage <- read.table(file.path(projFolder, "BAM", "alignments_finalMock", clusterFiles[1]))
names(clusterCoverage)[1:2] <- c("cluster", clusterFiles[1])

for(i in 2:length(clusterFiles)){
  tmp <- read.table(file.path(projFolder, "BAM", "alignments_finalMock", clusterFiles[i]))
  names(tmp)[1:2] <- c("cluster", clusterFiles[i])
  clusterCoverage <- merge(clusterCoverage, tmp, by="cluster")
}
names(clusterCoverage)[2:(length(clusterFiles)+1)] <- clusterFiles
clusterCoverage[,1] <- as.numeric(gsub("Cluster", "", clusterCoverage[,1]))
clusterCoverage <- clusterCoverage[order(clusterCoverage[,1]),]
clusterCoverage <- clusterCoverage[is.na(clusterCoverage[,1])==FALSE,]

clusterCoverage.std <- t(t(clusterCoverage)/apply(clusterCoverage,2,sum))*100000
```

### Lorenz curve
Concentration measure of reads against clusters

```{r final lorenz}
plot(cumsum(sort(clusterCoverage[,2] / sum(clusterCoverage[,2]))), type="l", xlab="Cluster", ylab="Concentration")
for(i in 3:ncol(clusterCoverage)){
  lines(cumsum(sort(clusterCoverage[,i] / sum(clusterCoverage[,i]))))
}
```


### Stats on coverage

These are the amounts of clusters with different samples. 

```{r final  cluster coverage stats}
coveredClusters <- apply(clusterCoverage[,-1]>0,1,sum)
barplot(table(coveredClusters), xlab="No. of different samples aligned to cluster", col=report.colours[1])
```

Now we have here the number of reads per coverage class. That means, instead of having it binary as in the previous plot, we now count all the reads per coverage group.

```{r final reads per coverage group}
readsPerCoverageGroup <- c()

for(i in 1:max(coveredClusters)){
 rowsOI <- which(coveredClusters==i)
 if(length(rowsOI)>0){
   readsPerCoverageGroup[i] <-  sum(clusterCoverage[rowsOI,-1])   
 } else {
   readsPerCoverageGroup[i] <-  0 
 }
}

names(readsPerCoverageGroup) <- 1:max(coveredClusters)

barplot(readsPerCoverageGroup, xlab="Coverage group", ylab="Reads on cluster group", col=report.colours[1])
```


```{r final reads per coverage group in percent}
barplot(readsPerCoverageGroup/sum(readsPerCoverageGroup)*100, xlab="Coverage group", ylab="Reads on cluster group (in %)", ylim=c(0,100), col=report.colours[1])
```

Then still the number of clusters without coverage per sample

```{r final clusters without coverage}
nonHittedClusters <- apply(clusterCoverage[,-1]==0,2,sum)
names(nonHittedClusters) <- gsub(".coverage", "", names(nonHittedClusters))
barplot(nonHittedClusters, col=report.colours[1], las=2)
lines(c(0,100000), c(nrow(clusterCoverage), nrow(clusterCoverage)), lty="dotted")
```

And then this still as percentage

```{r final clusterhits percentage}
barplot(nonHittedClusters/nrow(clusterCoverage), ylim=c(0,1), las=2, col=report.colours[1])
lines(c(0,10000),c(0.2,0.2), lty="dotted")
lines(c(0,10000),c(0.4,0.4), lty="dotted")
lines(c(0,10000),c(0.5,0.5), lty="dashed")
lines(c(0,10000),c(0.6,0.6), lty="dotted")
lines(c(0,10000),c(0.8,0.8), lty="dotted")
```

### Smoothed log-coverage per cluster

```{r final vizCusterCoverage}
plot(smooth.spline(clusterCoverage[,1], log(clusterCoverage[,2]+1), all.knots=FALSE), type="l", xlab="Cluster", ylab="Log-coverage", ylim=c(0, max(log(clusterCoverage))/2))

for(i in 3:ncol(clusterCoverage)){
  lines(smooth.spline(clusterCoverage[,1], log(clusterCoverage[,i]+1), all.knots=FALSE))  
}
```

### Smoothed std-log-coverage per cluster

Now the coverages are divided by the total amount of reads per sample and then multiplied by 10^5.

```{r final vizStdCusterCoverage}
# I use here the first column of the other matrix to keep the same coordinates. It does not affect the plot.
plot(smooth.spline(clusterCoverage[,1], log(clusterCoverage.std[,2]+1), all.knots=FALSE), type="l", xlab="Cluster", ylab="", ylim=c(0, max(log(clusterCoverage.std))/4))

for(i in 3:ncol(clusterCoverage)){
  lines(smooth.spline(clusterCoverage[,1], log(clusterCoverage.std[,i]+1), all.knots=FALSE))  
}
```



## Reference genome
### Basic stats
Import the mapping statistics to the reference genome

```{r import ref flagstats}
if(refAvail){
  flagstatFiles <- list.files(file.path(projFolder,"FASTQ", "TRIMMED", "alignments_reference"), pattern="*.flagstat")
  flagstats <- list()
  for(i in 1:length(flagstatFiles)){
    flagstats[[i]] <- readLines(file.path(projFolder,"FASTQ", "TRIMMED", "alignments_reference",flagstatFiles[i]))
  }
}
```

### Coverage
```{r reference mapping stat grpah}
par(oma=c(6,3,0,0))
mapStats <- matrix(0,ncol=length(flagstatFiles), nrow=2)

sampleNames <- gsub(".sam.flagstat", "", flagstatFiles)

colnames(mapStats) <- sampleNames

tmp <- as.numeric(sapply(strsplit(sapply(flagstats,"[",1), " +"),"[",1))
mapStats[1,] <- as.numeric(sapply(strsplit(sapply(flagstats,"[",5), " +"),"[",1))
mapStats[2,] <- tmp - mapStats[1,]

p <- barplot(mapStats, col=c(report.colours[1], report.colours[2]), las=2)

legend("topleft", pch=c(20,20), col=c(report.colours[2], report.colours[1]), legend=c("Unmapped", "Mapped"), fill="white")

# highlight the mock reference samples
mockFiles <- paste(mockSamples, ".sam.flagstat", sep="")
mockPos <- which(is.element(flagstatFiles, mockFiles))

points(p[mockPos], rep(200000, length(mockPos)), pch="*", col="red", cex=4)
```

```{r ref mapping percentage}
refGenome.mappingStats <- mapStats[1,] / (apply(mapStats,2,sum))
barplot(mapStats[1,] / (apply(mapStats,2,sum)), ylim=c(0,1), ylab="Mapping in Percent", col=report.colours[1], las=2)
```

The average mapping rate on the reference genome is `r mean(refGenome.mappingStats)`.

Now we check still the precise coverage per sample for the sake of simplicity I do it here for the first sample only and also only for the first at most 50 chromosome

```{r}
mpile_files <- list.files(file.path(projFolder, "MPILEUP", "mpileup_reference"), pattern="*.mpileup")
covData <- fread(file.path(projFolder, "MPILEUP", "mpileup_reference", mpile_files[10]), sep="\t")
```
```{r, fig.width=20, fig.height=10}
mpile_files <- list.files(file.path(projFolder, "MPILEUP", "mpileup_reference"), pattern="*.mpileup")

for(sam in 1:10){
  par(mfrow=c(4,10),
  mar=c(1,1,1,1))
  
  covData <- fread(file.path(projFolder, "MPILEUP", "mpileup_reference", mpile_files[sam]), sep="\t")
  chrNames <- names(table(covData[,1]))
  chrNames <- chrNames[1:min(49, length(chrNames))]

  maxChr <- min(length(chrNames), 20)
  
  for(i in 1:maxChr){
    tmpChr <- chrNames[i]
    
    tmpData <- covData[which(covData[,1]==tmpChr),]
    
     out <- stepMean(x=tmpData$V2, y=tmpData$V4, step=30000)
     plotcol <- rep("black", length(out))
     plotcol[out[,2]>20] <- "red"
     plotcol[out[,2]<10] <- "blue"
     
     if(is.element(i, c(1,11,21,31))){
        plot(out, col=plotcol, xaxt="n", ylim=c(0,40), main=paste0("Chr ",i))  
     } else {
        plot(out, col=plotcol, xaxt="n",yaxt="n", ylim=c(0,40), main=paste0("Chr ",i))     
     }
  }
}
```

### Read alignments
The aligned reads are interpreted as "transcripts" and via Stringtie novel transcripts are detected to identify the common alignments via stringtie merge

```{r}
string.merge <- importGTF(file.path(projFolder, "Stringtie", "merged_STRG.gtf"), level="transcript")
```

Length distribution of identified contigs across the reference genome

```{r}
  tmp <- summary(abs(string.merge$V5 - string.merge$V4))
  out <- data.frame(names(tmp), as.vector(tmp))
  out_html <- knitr::kable(out, col.names = NULL, "html")
  kable_styling(out_html, "striped", position = "left")
```

Here, we have the alignment length distribution of the reads in a histrogram. Axis labels are to be read in groups of 100, meaning a bar between 200 and 300 tells how many string merge loci had a total length between 200 and 300 bp.

```{r}
x <- abs(string.merge$V5 - string.merge$V4)
bin_width <- 100
nbins <- seq(min(x) - bin_width,
             max(x) + bin_width,
             by = bin_width)
hist(x, las=2, col=report.colours[1], breaks=nbins, xaxt="n", main="Alignment length distribution")
axis(1, nbins)
```

Then we have the identified clusters per chromosome on the reference genome

```{r}
barplot(table(string.merge$V1), las=2, col=report.colours[1])
```

### Individual assessments

Now we check for the alignments per sample

```{r import individual alignments, echo=FALSE, message=FALSE}
gtf.files <- list.files(file.path(projFolder, "Stringtie"), pattern="*.gtf$")
gtf.files <- gtf.files[-which(gtf.files=="merged_STRG.gtf")]

gtf.names <- gsub(".stringtie.gtf", "", gtf.files)

gtfs <- list()
gtfs[[1]] <- importGTF(file.path(projFolder, "Stringtie", gtf.files[1]), level="transcript")

samples_without_loci <- c()
remove_those <- c()
for(i in 2:length(gtf.files)){
  
  number_loci <- sum(substr(readLines(file.path(projFolder, "Stringtie", gtf.files[i]), n=10),1,1)!="#")
  if(number_loci>0){
    gtfs[[i]] <- importGTF(file.path(projFolder, "Stringtie", gtf.files[i]), level="transcript")
  } else {
    samples_without_loci <- c(samples_without_loci, gtf.names[i])
    remove_those <- c(remove_those, i)
  }
}

if(length(remove_those)>0){
  gtfs <- gtfs[-remove_those]
  names(gtfs) <- gtf.names[-remove_those]
} else {
  names(gtfs) <- gtf.names
}
```

Visualisation, how many alignment loci do the different samples have along the reference genome, ideally you see here a block that represents also the in-situ prediction for the amount of cut-sites with the
specific contig length after size-selection.

```{r, fig.width=10}
par(mar=c(7,5,0,0))
gtfRows <- sapply(gtfs, nrow)
barplot(gtfRows, las=2, col=report.colours[1])
```

And now the same with the overall amount of identified loci in relation

```{r, fig.width=10}
par(mar=c(7,5,0,0))
allLoci <- c(length(string.merge$V1), gtfRows)
names(allLoci) <- c("All loci", names(gtfRows))
barplot(allLoci, las=2, col=c(report.colours[2], rep(report.colours[1],length(gtfRows))))
```

And also the percentage, how many overall loci have alignments from each sample

```{r, fig.width=10}
par(mar=c(7,5,0,0))
barplot(allLoci/allLoci[1], las=2, col=c(report.colours[2], rep(report.colours[1],length(gtfRows))), ylim=c(0,1))
```


Now we will check how the reads from different samples distribute across the individual aligned loci (which have been merged together). Ideally, we want to see here that all samples have reads from all mapping loci, which is of course not possible but there should be "some consistency".

```{r}
fcFiles <- list.files(file.path(projFolder, "QUANTIFICATION", "Reference_contigs"), pattern="*.txt$")

fc.quant <- importFeatureCounts(file.path(projFolder, "QUANTIFICATION", "Reference_contigs", fcFiles[1]))
fc.names <- gsub("_reference_contigs_fc.txt", "", fcFiles)
for(i in 2:length(fcFiles)){
  tmp <- importFeatureCounts(file.path(projFolder, "QUANTIFICATION", "Reference_contigs", fcFiles[i]))
  
  fc.quant$expValues <- merge(fc.quant$expValues, tmp$expValues)
}

colnames(fc.quant$expValues) <- fc.names
```

Typically, we have plenty of loci, so a simple image is not feasible. Hence, we create here a image of the (Spearman-)correlation matrix between the loci. In case that the read alignments are equal, it would be one.

```{r, fig.width=10}
loci_cor <- cor(fc.quant$expValues[, -1], method="spearman")
image(loci_cor)
```

Then, we check the pair-wise similarities of loci between the samples. If two samples have common reads exactly in ll their alignment loci, the value would be 1, if they do not share a single locus, the value is 0

```{r}
loci_similarity <- matrix(0, ncol=ncol(fc.quant$expValues), nrow=ncol(fc.quant$expValues))
for(i in 1:(ncol(fc.quant$expValues)-1)){
  for(j in (i+1):ncol(fc.quant$expValues)){
    reads1 <- fc.quant$expValues[,i]>0
    reads2 <- fc.quant$expValues[,j]>0
    loci_similarity[i,j] <- sum((reads1 + reads2)==2)/length(reads1)
    loci_similarity[j,i] <- sum((reads1 + reads2)==2)/length(reads2)
  }
}
```

```{r}
image(loci_similarity)
```

Further, we need to check, how equally the reads are distributed per sample. For that, we calculate the Gini coefficient for each sample. In case that the reads are equally distributed across the alignment loci (per sample), that sample has a gini coefficient of one. In case that all reads originate from a single locus the Gini coefficient is roughly one.

```{r, eval=FALSE}
gini_sample <- c()
for(i in 1:ncol(fc.quant$expValues)){
  tmpExpr <- fc.quant$expValues[,i]
  tmpExpr <- tmpExpr[tmpExpr>0]
  gini_sample[i] <- gini(tmpExpr)
}

barplot(gini_sample)

```

NEED TO CONTINUE HERE STILL, THERE IS PLENTY OF ADDITIONAL VISUALISATIONS POSSIBLE!!!

TO BE ADDED:
* Visualisation, how are loci distributed across the chromosomes ("dots on chromosome lines")
* quantifications per contig, across samples
* flanking side sequences of the identified loci 
* Intersection with the mock reference contig alignments against reference genome
* ...

