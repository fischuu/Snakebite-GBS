# Alignment

## Preliminary Mock Reference
### Mapping stats
 
Here, we present the mapping statistics for the preliminary mock reference.

```{r import flagstats}
flagstatFiles <- list.files(file.path(projFolder,"FASTQ", "TRIMMED", "alignments"), pattern="*.flagstat")
flagstats <- list()
for(i in 1:length(flagstatFiles)){
  flagstats[[i]] <- readLines(file.path(projFolder,"FASTQ", "TRIMMED", "alignments",flagstatFiles[i]))
}
```

Visualization of the alignments, red stars indicate the samples that were used to build the mock reference.

```{r vis mapping stats}
par(oma=c(6,3,0,0))
mapStats <- matrix(0,ncol=length(flagstatFiles), nrow=2)

sampleNames <- gsub(".sam.flagstat", "", flagstatFiles)

colnames(mapStats) <- sampleNames

tmp <- as.numeric(sapply(strsplit(sapply(flagstats,"[",1), " +"),"[",1))
mapStats[1,] <- as.numeric(sapply(strsplit(sapply(flagstats,"[",5), " +"),"[",1))
mapStats[2,] <- tmp - mapStats[1,]

p <- barplot(mapStats, col=c(report.colours[1], report.colours[2]), las=2)

legend("topleft", pch=c(20,20), col=c(report.colours[2], report.colours[1]), legend=c("Unmapped", "Mapped"), fill="white")

# highlight the mock reference samples
mockFiles <- paste(mockSamples, ".sam.flagstat", sep="")
mockPos <- which(is.element(flagstatFiles, mockFiles))

points(p[mockPos], rep(200000, length(mockPos)), pch="*", col="red", cex=4)
```

```{r mapping percentage}
barplot(mapStats[1,] / (apply(mapStats,2,sum)) * 100, ylim=c(0,100), ylab="Mapping in Percent", col=report.colours[1], las=2)
```

And the same information in table format.

```{r}
mappingRates <- mapStats[1,] / apply(mapStats,2,sum)
out <- as.matrix(mappingRates)
colnames(out)[1] <- "Mapping rate"

tmp <- cbind(colnames(mapStats), out[,1])

if(nrow(tmp)>0){
  rownames(tmp) <- 1:nrow(tmp)
  colnames(tmp) <- c("Sample", "Alignment rate")
  #knitr::kable(tmp) %>% kable_styling
  datatable(tmp, extensions = 'Buttons',
            options = list(dom = 'Blfrtip',
                           buttons = c('copy', 'csv', 'excel', 'pdf', 'print'),
                           lengthMenu = list(c(10,25,50,-1),
                                             c(10,25,50,"All"))))
}
```

The average mapping rate is `r mean(mappingRates)`.

### Coverage

Here, the coverage statistics for reads mapped against the clusters are presented.

```{r importClusterCoverage, warning=FALSE}
clusterFiles <- list.files(file.path(projFolder, "FASTQ", "TRIMMED", "alignments_clusters"), pattern="*.coverage")
clusterCoverage <- read.table(file.path(projFolder, "FASTQ", "TRIMMED", "alignments_clusters", clusterFiles[1]))
names(clusterCoverage)[1:2] <- c("cluster", clusterFiles[1])

for(i in 2:length(clusterFiles)){
  tmp <- read.table(file.path(projFolder, "FASTQ", "TRIMMED", "alignments_clusters", clusterFiles[i]))
  names(tmp)[1:2] <- c("cluster", clusterFiles[i])
  clusterCoverage <- merge(clusterCoverage, tmp, by="cluster")
}
names(clusterCoverage)[2:(length(clusterFiles)+1)] <- clusterFiles
clusterCoverage[,1] <- as.numeric(gsub("Cluster", "", clusterCoverage[,1]))
clusterCoverage <- clusterCoverage[order(clusterCoverage[,1]),]
clusterCoverage <- clusterCoverage[is.na(clusterCoverage[,1])==FALSE,]

clusterCoverage.std <- t(t(clusterCoverage)/apply(clusterCoverage,2,sum))*100000
```

### Lorenz curve
The Lorenz curve is a graphical representation of the distribution of a particular variable, here for example reads. It is used to visualize and measure read coverage inequality within the alignments of a sample.

To create a Lorenz curve, the clusters are ranked from lowest to highest coverage. The cumulative percentage of the reads are then plotted on the y-axis, while the cumulative percentage of the clusters is plotted on the x-axis.

A perfectly equal distribution of the reads across the different clusters would be represented by a straight diagonal line from the bottom left to the top right of the graph, indicating that each cluster has the same percentage of the total read number
aligned. In contrast, a curve that is further from the diagonal line indicates a greater degree of inequality, with a smaller portion of the clusters gathering a larger proportion of the total read coverage.

```{r lorenz}
plot(cumsum(sort(clusterCoverage[,2] / sum(clusterCoverage[,2]))), type="l", xlab="Sorted clusters", ylab="Concentration")
for(i in 3:ncol(clusterCoverage)){
  lines(cumsum(sort(clusterCoverage[,i] / sum(clusterCoverage[,i]))))
}
```

### Stats on coverage

These are the amounts of clusters with reads from different samples. For example, the bar at 4 on the x-axis gives the amount of clusters with reads from four different samples. Naturally, one would like to see here as high as possible bars on the right hand side of the plot.

```{r cluster coverage stats}
coveredClusters <- apply(clusterCoverage[,-1]>0,1,sum)
barplot(table(coveredClusters), xlab="No. of different samples aligned to cluster", col=report.colours[1])
```

Now we have here the number of reads per coverage class. That means, instead of having it binary as in the previous plot, we now count all the reads per coverage group, which is more representative for the true coverage.

```{r reads per coverage group}
readsPerCoverageGroup <- c()

for(i in 1:max(coveredClusters)){
  if(sum(coveredClusters==i)>0){
    readsPerCoverageGroup[i] <-  sum(clusterCoverage[coveredClusters==i,-1])
  } else {
    readsPerCoverageGroup[i] <- 0
  }
}

names(readsPerCoverageGroup) <- 1:max(coveredClusters)

barplot(readsPerCoverageGroup, xlab="Coverage group", ylab="Reads on cluster group", col=report.colours[1])
```
And the same still with percentages

```{r reads per coverage group in percent}
barplot(readsPerCoverageGroup/sum(readsPerCoverageGroup)*100, xlab="Coverage group", ylab="Reads on cluster group (in %)", ylim=c(0,100), col=report.colours[1])
```

Then still the number of clusters without coverage per sample

```{r clusters without coverage}
par(mar=c(7,4,1,1))
nonHittedClusters <- apply(clusterCoverage[,-1]==0,2,sum)
names(nonHittedClusters) <- gsub(".coverage", "", names(nonHittedClusters))
barplot(nonHittedClusters, col=report.colours[1], las=2)
lines(c(0,100000), c(nrow(clusterCoverage), nrow(clusterCoverage)), lty="dotted")
```

And then also this as percentage

```{r clusterhits percentage}
barplot(nonHittedClusters/nrow(clusterCoverage), ylim=c(0,1), las=2, col=report.colours[1])
lines(c(0,10000),c(0.2,0.2), lty="dotted")
lines(c(0,10000),c(0.4,0.4), lty="dotted")
lines(c(0,10000),c(0.5,0.5), lty="dashed")
lines(c(0,10000),c(0.6,0.6), lty="dotted")
lines(c(0,10000),c(0.8,0.8), lty="dotted")
```


### Smoothed log-coverage per cluster

```{r vizCusterCoverage}
plot(smooth.spline(clusterCoverage[,1], log(clusterCoverage[,2]+1), all.knots=FALSE), type="l", xlab="Cluster", ylab="Log-coverage", ylim=c(0, max(log(clusterCoverage))/2))

for(i in 3:ncol(clusterCoverage)){
  lines(smooth.spline(clusterCoverage[,1], log(clusterCoverage[,i]+1), all.knots=FALSE))  
}
```

### Smoothed std-log-coverage per cluster

Now the coverages are divided by the total amount of reads per sample and then multiplied by 10^5.

```{r vizStdCusterCoverage}
# I use here the first column of the other matrix to keep the same coordinates. It does not affect the plot.
plot(smooth.spline(clusterCoverage[,1], log(clusterCoverage.std[,2]+1), all.knots=FALSE), type="l", xlab="Cluster", ylab="", ylim=c(0, max(log(clusterCoverage.std))/4))

for(i in 3:ncol(clusterCoverage)){
  lines(smooth.spline(clusterCoverage[,1], log(clusterCoverage.std[,i]+1), all.knots=FALSE))  
}
```

## Mock Reference
### Mapping stats

```{r import final flagstats}
# Import the mapping statistics to the Mock reference
flagstatFiles <- list.files(file.path(projFolder,"BAM", "alignments_finalMock"), pattern="*.flagstat")
flagstats <- list()
for(i in 1:length(flagstatFiles)){
  flagstats[[i]] <- readLines(file.path(projFolder,"BAM", "alignments_finalMock",flagstatFiles[i]))
}
```

Visualization of the alignments, red stars indicate the mock reference samples

```{r vis mapping final stats}
par(oma=c(6,3,0,0))
mapStats <- matrix(0,ncol=length(flagstatFiles), nrow=2)

sampleNames <- gsub(".sam.flagstat", "", flagstatFiles)

colnames(mapStats) <- sampleNames

tmp <- as.numeric(sapply(strsplit(sapply(flagstats,"[",1), " +"),"[",1))
mapStats[1,] <- as.numeric(sapply(strsplit(sapply(flagstats,"[",5), " +"),"[",1))
mapStats[2,] <- tmp - mapStats[1,]

p <- barplot(mapStats, col=c(report.colours[1], report.colours[2]), las=2)

legend("topleft", pch=c(20,20), col=c(report.colours[2], report.colours[1]), legend=c("Unmapped", "Mapped"), fill="white")

# highlight the mock reference samples
mockFiles <- paste(mockSamples, ".sam.flagstat", sep="")
mockPos <- which(is.element(flagstatFiles, mockFiles))

points(p[mockPos], rep(200000, length(mockPos)), pch="*", col="red", cex=4)
```

```{r mapping final percentage}
barplot(mapStats[1,] / (apply(mapStats,2,sum)) * 100, ylim=c(0,100), ylab="Mapping in Percent", col=report.colours[1], las=2)
```

And the same information in table format.

```{r}
mappingRates <- mapStats[1,] / apply(mapStats,2,sum)
out <- as.matrix(mappingRates)
colnames(out)[1] <- "Mapping rate"
tmp <- cbind(colnames(mapStats), out[,1])

if(nrow(tmp)>0){
  rownames(tmp) <- 1:nrow(tmp)
  colnames(tmp) <- c("Sample", "Alignment rate")
  #knitr::kable(tmp) %>% kable_styling
  datatable(tmp, extensions = 'Buttons',
            options = list(dom = 'Blfrtip',
                           buttons = c('copy', 'csv', 'excel', 'pdf', 'print'),
                           lengthMenu = list(c(10,25,50,-1),
                                             c(10,25,50,"All"))))
}
```

The average mapping rate is `r mean(mappingRates)`.

### Coverage

Data mapped against the clusters and then the reads per cluster visualized

```{r importFinalClusterCoverage, warning=FALSE}
clusterFiles <- list.files(file.path(projFolder, "BAM", "alignments_finalMock"), pattern="*.coverage")
clusterCoverage <- read.table(file.path(projFolder, "BAM", "alignments_finalMock", clusterFiles[1]))
names(clusterCoverage)[1:2] <- c("cluster", clusterFiles[1])

for(i in 2:length(clusterFiles)){
  tmp <- read.table(file.path(projFolder, "BAM", "alignments_finalMock", clusterFiles[i]))
  names(tmp)[1:2] <- c("cluster", clusterFiles[i])
  clusterCoverage <- merge(clusterCoverage, tmp, by="cluster")
}
names(clusterCoverage)[2:(length(clusterFiles)+1)] <- clusterFiles
clusterCoverage[,1] <- as.numeric(gsub("Cluster", "", clusterCoverage[,1]))
clusterCoverage <- clusterCoverage[order(clusterCoverage[,1]),]
clusterCoverage <- clusterCoverage[is.na(clusterCoverage[,1])==FALSE,]

clusterCoverage.std <- t(t(clusterCoverage)/apply(clusterCoverage,2,sum))*100000
```

### Lorenz curve

The Lorenz curve is a graphical representation of the distribution of a particular variable, here for example reads. It is used to visualize and measure read coverage inequality within the alignments of a sample.

To create a Lorenz curve, the clusters are ranked from lowest to highest coverage. The cumulative percentage of the reads are then plotted on the y-axis, while the cumulative percentage of the clusters is plotted on the x-axis.

A perfectly equal distribution of the reads across the different clusters would be represented by a straight diagonal line from the bottom left to the top right of the graph, indicating that each cluster has the same percentage of the total read number
aligned. In contrast, a curve that is farther from the diagonal line indicates a greater degree of inequality, with a smaller portion of the clusters gathering a larger proportion of the total read coverage.


```{r final lorenz}
plot(cumsum(sort(clusterCoverage[,2] / sum(clusterCoverage[,2]))), type="l", xlab="Cluster", ylab="Concentration")
for(i in 3:ncol(clusterCoverage)){
  lines(cumsum(sort(clusterCoverage[,i] / sum(clusterCoverage[,i]))))
}
```


### Stats on coverage

These are the amounts of clusters with different samples. 

```{r final  cluster coverage stats}
coveredClusters <- apply(clusterCoverage[,-1]>0,1,sum)
barplot(table(coveredClusters), xlab="No. of different samples aligned to cluster", col=report.colours[1])
```

Now we have here the number of reads per coverage class. That means, instead of having it binary as in the previous plot, we now count all the reads per coverage group.

```{r final reads per coverage group}
readsPerCoverageGroup <- c()

for(i in 1:max(coveredClusters)){
 rowsOI <- which(coveredClusters==i)
 if(length(rowsOI)>0){
   readsPerCoverageGroup[i] <-  sum(clusterCoverage[rowsOI,-1])   
 } else {
   readsPerCoverageGroup[i] <-  0 
 }
}

names(readsPerCoverageGroup) <- 1:max(coveredClusters)

barplot(readsPerCoverageGroup, xlab="Coverage group", ylab="Reads on cluster group", col=report.colours[1])
```


```{r final reads per coverage group in percent}
barplot(readsPerCoverageGroup/sum(readsPerCoverageGroup)*100, xlab="Coverage group", ylab="Reads on cluster group (in %)", ylim=c(0,100), col=report.colours[1])
```

Then still the number of clusters without coverage per sample

```{r final clusters without coverage}
par(mar=c(7,4,1,1))
nonHittedClusters <- apply(clusterCoverage[,-1]==0,2,sum)
names(nonHittedClusters) <- gsub(".coverage", "", names(nonHittedClusters))
barplot(nonHittedClusters, col=report.colours[1], las=2)
lines(c(0,100000), c(nrow(clusterCoverage), nrow(clusterCoverage)), lty="dotted")
```

And then this still as percentage

```{r final clusterhits percentage}
par(mar=c(7,4,1,1))
barplot(nonHittedClusters/nrow(clusterCoverage), ylim=c(0,1), las=2, col=report.colours[1])
lines(c(0,10000),c(0.2,0.2), lty="dotted")
lines(c(0,10000),c(0.4,0.4), lty="dotted")
lines(c(0,10000),c(0.5,0.5), lty="dashed")
lines(c(0,10000),c(0.6,0.6), lty="dotted")
lines(c(0,10000),c(0.8,0.8), lty="dotted")
```

### Smoothed log-coverage per cluster

```{r final vizCusterCoverage}
plot(smooth.spline(clusterCoverage[,1], log(clusterCoverage[,2]+1), all.knots=FALSE), type="l", xlab="Cluster", ylab="Log-coverage", ylim=c(0, max(log(clusterCoverage))/2))

for(i in 3:ncol(clusterCoverage)){
  lines(smooth.spline(clusterCoverage[,1], log(clusterCoverage[,i]+1), all.knots=FALSE))  
}
```

### Smoothed std-log-coverage per cluster

Now the coverages are divided by the total amount of reads per sample and then multiplied by 10^5.

```{r final vizStdCusterCoverage}
# I use here the first column of the other matrix to keep the same coordinates. It does not affect the plot.
plot(smooth.spline(clusterCoverage[,1], log(clusterCoverage.std[,2]+1), all.knots=FALSE), type="l", xlab="Cluster", ylab="", ylim=c(0, max(log(clusterCoverage.std))/4))

for(i in 3:ncol(clusterCoverage)){
  lines(smooth.spline(clusterCoverage[,1], log(clusterCoverage.std[,i]+1), all.knots=FALSE))  
}
```



## Reference genome
### Basic stats

```{r import ref flagstats}
if(refAvail){
  flagstatFiles <- list.files(file.path(projFolder,"FASTQ", "TRIMMED", "alignments_reference"), pattern="*.flagstat")
  flagstats <- list()
  for(i in 1:length(flagstatFiles)){
    flagstats[[i]] <- readLines(file.path(projFolder,"FASTQ", "TRIMMED", "alignments_reference",flagstatFiles[i]))
  }
}
```

### Coverage
```{r reference mapping stat grpah}
par(oma=c(6,3,0,0))
mapStats <- matrix(0,ncol=length(flagstatFiles), nrow=2)

sampleNames <- gsub(".sam.flagstat", "", flagstatFiles)

colnames(mapStats) <- sampleNames

tmp <- as.numeric(sapply(strsplit(sapply(flagstats,"[",1), " +"),"[",1))
mapStats[1,] <- as.numeric(sapply(strsplit(sapply(flagstats,"[",5), " +"),"[",1))
mapStats[2,] <- tmp - mapStats[1,]

p <- barplot(mapStats, col=c(report.colours[1], report.colours[2]), las=2)

legend("topleft", pch=c(20,20), col=c(report.colours[2], report.colours[1]), legend=c("Unmapped", "Mapped"), fill="white")

# highlight the mock reference samples
mockFiles <- paste(mockSamples, ".sam.flagstat", sep="")
mockPos <- which(is.element(flagstatFiles, mockFiles))

points(p[mockPos], rep(200000, length(mockPos)), pch="*", col="red", cex=4)
```

```{r ref mapping percentage}
refGenome.mappingStats <- mapStats[1,] / (apply(mapStats,2,sum))
barplot(mapStats[1,] / (apply(mapStats,2,sum)), ylim=c(0,1), ylab="Mapping in Percent", col=report.colours[1], las=2)
```
And the same information in table format.

```{r}
mappingRates <- mapStats[1,] / apply(mapStats,2,sum)
out <- as.matrix(mappingRates)
colnames(out)[1] <- "Mapping rate"
tmp <- cbind(colnames(mapStats), out[,1])

if(nrow(tmp)>0){
  rownames(tmp) <- 1:nrow(tmp)
  colnames(tmp) <- c("Sample", "Alignment rate")
  #knitr::kable(tmp) %>% kable_styling
  datatable(tmp, extensions = 'Buttons',
            options = list(dom = 'Blfrtip',
                           buttons = c('copy', 'csv', 'excel', 'pdf', 'print'),
                           lengthMenu = list(c(10,25,50,-1),
                                             c(10,25,50,"All"))))
}
```

The average mapping rate on the reference genome is `r mean(refGenome.mappingStats)`.

Now we check still the precise coverage (in terms of alignment loci) per sample for the sake of simplicity we'll do it here for the first sample only and also only for the first 40 chromosomes (if there are as many...). Here, one can possibly see issues in the reference genome, when the coverage is not uniformly distributed. Areas with a coverage of more than 20 reads are indicated in red, those below 10 are in blue and areas in between are represented with black.

```{r, fig.width=20, fig.height=10}
mpile_files <- list.files(file.path(projFolder, "MPILEUP", "mpileup_reference"), pattern="*.mpileup")

# Chose the best sam, with the largest file
sam <- 1
fileSize <- -1
for(i in 1:length(mpile_files)){
  fileInfo <- file.info(file.path(projFolder, "MPILEUP", "mpileup_reference", mpile_files[i]))  
  if(fileInfo$size>fileSize){
    sam <- i
    fileSize <- fileInfo$size
  }
}





  covData <- fread(file.path(projFolder, "MPILEUP", "mpileup_reference", mpile_files[sam]), sep="\t")
  chrNames <- names(table(covData[,1]))
  chrNames <- chrNames[1:min(49, length(chrNames))]

  maxChr <- min(length(chrNames), 40)

  par(mfrow=c(maxChr%/%10+1,10),
  mar=c(1,1,1,1))
    
  for(i in 1:maxChr){
    tmpChr <- chrNames[i]
    
    tmpData <- covData[which(covData[,1]==tmpChr),]
    
     out <- stepMean(x=tmpData$V2, y=tmpData$V4, step=30000)
     plotcol <- rep("black", length(out))
     plotcol[out[,2]>20] <- "red"
     plotcol[out[,2]<10] <- "blue"
     
     if(is.element(i, c(1,11,21,31))){
        plot(out, col=plotcol, xaxt="n", ylim=c(0,40), main=paste0("Chr ",i))  
     } else {
        plot(out, col=plotcol, xaxt="n",yaxt="n", ylim=c(0,40), main=paste0("Chr ",i))     
     }
  }
  
```

### Read alignments
The aligned reads are now interpreted as "transcripts" and via Stringtie novel transcripts are detected to identify the common alignments via stringtie merge.

```{r, echo=FALSE, message=FALSE, results=FALSE}
string.merge <- importGTF(file.path(projFolder, "Stringtie", "merged_STRG.gtf"), level="transcript")
```

Length distribution of the identified contigs across the reference genome. The identification takes place, just by aligning the reads against the reference genome, as each read should represent here a cut-site.
Please note, this distribution is based on the Stringtie merge output, meaning, each location is counted once, and is NOT weighted based on the coverage, so outliers have the same weight as well-covered areas.

```{r}
  tmp <- summary(abs(string.merge$V5 - string.merge$V4))
  out <- data.frame(names(tmp), as.vector(tmp))
  out_html <- knitr::kable(out, col.names = NULL, "html")
  kable_styling(out_html, "striped", position = "left")
```

Here, we have the alignment length distribution of the reads in a histrogram. Axis labels are to be read in groups of 100, meaning a bar between 200 and 300 tells how many string merge loci had a total length between 200 and 300 bp.

```{r}
x <- abs(string.merge$V5 - string.merge$V4)
bin_width <- 100
nbins <- seq(0,
             max(x) + bin_width,
             by = bin_width)
hist(x, las=2, col=report.colours[1], breaks=nbins, xaxt="n", main="Alignment length distribution (unweighted)")
axis(1, nbins)
```


```{r}
# Import the coverages to weight the locations properly
fcFiles <- list.files(file.path(projFolder, "QUANTIFICATION", "Reference_contigs"), pattern="*.txt$")

fc.quant <- importFeatureCounts(file.path(projFolder, "QUANTIFICATION", "Reference_contigs", fcFiles[1]))
fc.names <- gsub("_reference_contigs_fc.txt", "", fcFiles)
for(i in 2:length(fcFiles)){
  tmp <- importFeatureCounts(file.path(projFolder, "QUANTIFICATION", "Reference_contigs", fcFiles[i]))
  
  fc.quant$expValues <- merge(fc.quant$expValues, tmp$expValues)
}

colnames(fc.quant$expValues) <- c("Locus", fc.names)
```

```{r}
locationQuant <- merge(string.merge, fc.quant$expValues, by.x="gene_id", by.y="Locus")
locationQuant$length <- abs(locationQuant$V5 - locationQuant$V4)
locationQuant$sum <- apply(locationQuant[,12:(ncol(locationQuant)-1)],1,sum)

weightedLengths <- rep(locationQuant$length, locationQuant$sum)
```

And then the same distribution, but still weighted by the reads, so here we take each read into account, to determine the length distribution (this is the distribtion, you should look at when you check the QC of the sequencing run)

```{r}
  tmp <- summary(weightedLengths)
  out <- data.frame(names(tmp), as.vector(tmp))
  out_html <- knitr::kable(out, col.names = NULL, "html")
  kable_styling(out_html, "striped", position = "left")
```

Here, we have the alignment length distribution of the reads in a histrogram. Axis labels are to be read in groups of 100, meaning a bar between 200 and 300 tells how many string merge loci had a total length between 200 and 300 bp.

```{r}
x <- abs(weightedLengths)
bin_width <- 100
nbins <- seq(0,
             max(x) + bin_width,
             by = bin_width)
hist(x, las=2, col=report.colours[1], breaks=nbins, xaxt="n", main="Alignment length distribution (weighted)")
axis(1, nbins)
```

Then we have the identified clusters per chromosome on the reference genome

```{r}
barplot(table(string.merge$V1), las=2, col=report.colours[1])
```

### Individual assessments

Now we check for the alignments per sample

```{r import individual alignments, echo=FALSE, message=FALSE, results=FALSE}
gtf.files <- list.files(file.path(projFolder, "Stringtie"), pattern="*.gtf$")
gtf.files <- gtf.files[-which(gtf.files=="merged_STRG.gtf")]

gtf.names <- gsub(".stringtie.gtf", "", gtf.files)

gtfs <- list()
gtfs[[1]] <- importGTF(file.path(projFolder, "Stringtie", gtf.files[1]), level="transcript")

samples_without_loci <- c()
remove_those <- c()
for(i in 2:length(gtf.files)){
  
  number_loci <- sum(substr(readLines(file.path(projFolder, "Stringtie", gtf.files[i]), n=10),1,1)!="#")
  if(number_loci>0){
    gtfs[[i]] <- importGTF(file.path(projFolder, "Stringtie", gtf.files[i]), level="transcript")
  } else {
    samples_without_loci <- c(samples_without_loci, gtf.names[i])
    remove_those <- c(remove_those, i)
  }
}

if(length(remove_those)>0){
  gtfs <- gtfs[-remove_those]
  names(gtfs) <- gtf.names[-remove_those]
} else {
  names(gtfs) <- gtf.names
}
```

Visualisation, how many alignment loci do the different samples have along the reference genome, ideally you see here a block that represents also the in-situ prediction for the amount of cut-sites with the
specific contig length after size-selection.

```{r}
par(mar=c(7,5,0,0))
gtfRows <- sapply(gtfs, nrow)
barplot(gtfRows, las=2, col=report.colours[1])
```

And now the same with the overall amount of identified loci in relation

```{r, fig.width=10}
par(mar=c(7,5,0,0))
allLoci <- c(length(string.merge$V1), gtfRows)
names(allLoci) <- c("All loci", names(gtfRows))
barplot(allLoci, las=2, col=c(report.colours[2], rep(report.colours[1],length(gtfRows))))
```

And also the percentage, how many overall loci have alignments from each sample

```{r, fig.width=10}
par(mar=c(7,5,0,0))
barplot(allLoci/allLoci[1], las=2, col=c(report.colours[2], rep(report.colours[1],length(gtfRows))), ylim=c(0,1))
```


Now we will check how the reads from different samples distribute across the individual aligned loci (which have been merged together). Ideally, we want to see here that all samples have reads from all mapping loci, which is of course not possible but there should be "some consistency".

Visualisation of the pair-wise expression values. As this can be rather large, we do it for the first 4 samples only

```{r, fig.width=10}
tmp <- fc.quant$expValues[,2:min(ncol(fc.quant$expValues), 4)]
tmp <- tmp[apply(tmp, 1, sum)>0,]
pairs(tmp)
```

Concentration measure of reads against merged loci.

```{r final lorenz}
plot(cumsum(sort(fc.quant$expValues[,2] / sum(fc.quant$expValues[,2]))), type="l", xlab="Cluster", ylab="Concentration")
for(i in 3:ncol(fc.quant$expValues)){
  lines(cumsum(sort(fc.quant$expValues[,i] / sum(fc.quant$expValues[,i]))))
}
```

Concentration measure of reads against merged loci, without zero-locations per sample.

```{r final lorenz}
tmpData <- fc.quant$expValues[,2]
tmpData <- tmpData[tmpData!=0]
plot(cumsum(sort(tmpData / sum(tmpData))), type="l", xlab="Cluster", ylab="Concentration", xlim=c(0, max(apply(fc.quant$expValues[,-1]>0,2,sum))))

for(i in 3:ncol(fc.quant$expValues)){
    tmpData <- fc.quant$expValues[,i]
    tmpData <- tmpData[tmpData!=0]

    lines(cumsum(sort(tmpData / sum(tmpData))))
}
```

A table of the most covered locations

```{r}
tmp <- fc.quant$expValues
tmp <- merge(fc.quant$geneInfo[,-c(5)], tmp, by.x="Geneid", by.y = "Locus")
totCov <- apply(tmp[,-c(1:5)],1,sum)
rownames(tmp) <- fc.quant$expValues[,1]
tmp <- tmp[order(totCov, decreasing = TRUE)[1:50],]
if(nrow(tmp)>0){
  rownames(tmp) <- NULL
#  colnames(tmp) <- c("Sample", "Total sequences")
  #knitr::kable(tmp) %>% kable_styling
  datatable(tmp, extensions = 'Buttons', rownames=FALSE,
            options = list(dom = 'Blfrtip',
                           buttons = c('copy', 'csv', 'excel', 'pdf', 'print'),
                           lengthMenu = list(c(10,25,50,-1),
                                             c(10,25,50,"All"))))
}
```

Usually locations with lots of coverage are not considered. We check, how many reads those locations accumulate and if we loose many reads to them. Here, we check locations with a higher coverage than 200 reads.

```{r}
hcLocations <- vector(mode="numeric", length=ncol(fc.quant$expValues)-1)
hcCoverage <- vector(mode="numeric", length=ncol(fc.quant$expValues)-1)
names(hcLocations) <- colnames(fc.quant$expValues)[-1]
names(hcCoverage) <- colnames(fc.quant$expValues)[-1]

for(i in 2:ncol(fc.quant$expValues)){
  hcLocations[i-1] <- sum(fc.quant$expValues[,i]>200)
  hcCoverage[i-1] <- sum(fc.quant$expValues[fc.quant$expValues[,i]>200,i])
}

```

```{r, fig.width=10}
par(mar=c(8,4,2,0))
barplot(hcLocations , col=report.colours[1], las=2, main="High-coverage locations")
```

This one in relation to the total sequencing depth

```{r}
plot(hcLocations, totalTrimmedSeq, col=report.colours[1], xlab="High-coverage locations", ylab="Total trimmed sequences")
```

```{r, fig.width=10}
par(mar=c(8,5,2,0))
barplot(hcCoverage , col=report.colours[1], las=2, main="High-coverage location reads")
```

```{r, fig.width=10}
par(mar=c(8,5,2,0))
plotData <- hcCoverage/apply(fc.quant$expValues[,-1],2,sum)*100
barplot(plotData , col=report.colours[1], las=2, main="High-coverage location reads (in percent)")
```

Set that into relation to sequencing depth

```{r}
tmp <- cbind(plotData, totalTrimmedSeq)
plot(tmp, col=report.colours[1], xlab="Reads in high-coverage loci (in %)", ylab="Total trimmed sequencing depth")
```

Display how the reads and loci harmonise. For that we create a dataframe that displays the following. Each columns says what is the minimum number of samples with that feature and the
rows are the number of reads of a certain locus. That means, if e.g. row number 4 and column 6 says 38,931 that would mean, there are 38,931 loci where six or more samples have at least
four reads.

```{r}
readDist <- matrix(0, ncol=ncol(fc.quant$expValues), nrow=201)

for(i in 1:201){
  readDist[i,] <- table(factor(apply(fc.quant$expValues[,-1]>(i-1),1,sum), levels=0:(ncol(fc.quant$expValues)-1)))
}
```

```{r}
rdn <- ncol(readDist)
out <- t(apply(readDist[1:50,rdn:1],1,cumsum))[,rdn:1]

colnames(out) <- 0:(rdn-1)
rownames(out) <- 1:50
knitr::kable(out, row.names = TRUE)%>%  
 kable_styling(bootstrap_options = c("hover", "condensed", "responsive"), 
  latex_options = c("striped", "condensed", "HOLD_position", "repeat_header"),
  repeat_header_text = "(huhu)",
  full_width = F, fixed_thead = T) %>%  
 row_spec(0, bold=T, monospace=F, color="white", background="#7B7B7B")
```

Typically, we have plenty of loci, so a simple image is not feasible. Hence, we create here a image of the (Spearman-)correlation matrix between the loci. In case that the read alignments are equal, it would be one.

```{r, fig.width=10}
loci_cor <- cor(fc.quant$expValues[, -1], method="spearman")
image(loci_cor)
```

Then, we check the pair-wise similarities of loci between the samples. If two samples have common reads exactly in all their alignment loci, the value would be 1, if they do not share a single locus, the value is 0

```{r}
loci_similarity <- matrix(0, ncol=ncol(fc.quant$expValues), nrow=ncol(fc.quant$expValues))
for(i in 1:(ncol(fc.quant$expValues)-1)){
  for(j in (i+1):ncol(fc.quant$expValues)){
    reads1 <- fc.quant$expValues[,i]>0
    reads2 <- fc.quant$expValues[,j]>0
    loci_similarity[i,j] <- sum((reads1 + reads2)==2)/length(reads1)
    loci_similarity[j,i] <- sum((reads1 + reads2)==2)/length(reads2)
  }
}
```

```{r}
image(loci_similarity)
```

### Alignment flanking sites

We consider the flanking sites of the aligned reads. Please keep in mind that we align trimmed data, so this table might be misleading.

```{r}
out <- data.frame(Enzyme1 = enz1,
                  Enzyme1_reverse = revcomp(enz1),
                  Enzyme2 = enz2,
                  Enzyme2_reverse = revcomp(enz2))
rownames(out) <- c("Sequence")
out_html <- knitr::kable(t(out), "html")
kable_styling(out_html, "striped", position = "left")
```

```{r}
flankingLeft.list <- list.files(file.path(projFolder,"FASTQ", "TRIMMED", "alignments_reference"), pattern="*left.fa")
flankingRight.list <- list.files(file.path(projFolder,"FASTQ", "TRIMMED", "alignments_reference"), pattern="*right.fa")
flankingLeft <- list()
flankingRight <- list()

for(i in 1:length(flankingLeft.list)){
  fileInfo <- file.info(file.path(projFolder,"FASTQ", "TRIMMED", "alignments_reference", flankingLeft.list[i]))
  if(fileInfo$size>0){
    flankingLeft[[i]] <- importFA(file.path(projFolder,"FASTQ", "TRIMMED", "alignments_reference", flankingLeft.list[i]))
    flankingRight[[i]] <- importFA(file.path(projFolder,"FASTQ", "TRIMMED", "alignments_reference", flankingRight.list[i]))
  }
}


```

First, the distributions per sample of the most abundant flanking sequence (top-10)

#### Flanking sequences 'left'

This table is as we take it from the data, the most abundant sequences at the left site of the aligned reads.

```{r}
flankSeqLeft <- list()
for(i in 1:length(flankingLeft))
flankSeqLeft[[i]] <- sort(table(flankingLeft[[i]]), decreasing = TRUE)[1:10]/length(flankingLeft[[i]])
```

```{r}
out <- paste0(names(flankSeqLeft[[1]]), " - ",round(flankSeqLeft[[1]]*100,1),"%")
for(i in 2:length(flankSeqLeft)){
  out <- cbind(out, paste0(names(flankSeqLeft[[i]]), " - ",round(flankSeqLeft[[i]]*100,1),"%"))
}
colnames(out) <- gsub(".flanking_left.fa","",flankingLeft.list)
data.table(out)
```
#### Flanking sequences 'right'

Those are the most abundant sequences that we find at the right site of the aligned reads.

```{r}
flankSeqRight <- list()
for(i in 1:length(flankingRight))
flankSeqRight[[i]] <- sort(table(flankingRight[[i]]), decreasing = TRUE)[1:10]/length(flankingRight[[i]])
```

```{r}
out <- paste0(names(flankSeqRight[[1]]), " - ",round(flankSeqRight[[1]]*100,1),"%")
for(i in 2:length(flankSeqRight)){
  out <- cbind(out, paste0(names(flankSeqRight[[i]]), " - ",round(flankSeqRight[[i]]*100,1),"%"))
}
colnames(out) <- gsub(".flanking_right.fa","",flankingRight.list)
data.table(out)

```

#### Combined left and right sequences

And finally still the top10 combinations

```{r}

tmp <- paste(flankingLeft[[1]], flankingRight[[1]], sep="-")
tmp2 <- sort(table(tmp), decreasing=TRUE)[1:20]/length(tmp)
out <- paste0(names(tmp2), " - ",round(tmp2*100,1),"%")

for(i in 2:length(flankingLeft)){
   tmp <- paste(flankingLeft[[i]], flankingRight[[i]], sep="-")
   tmp2 <- sort(table(tmp), decreasing=TRUE)[1:20]/length(tmp)
   out <- cbind(out, paste0(names(tmp2), " - ",round(tmp2*100,1),"%"))
}
colnames(out) <- gsub(".flanking_right.fa","",flankingRight.list)
data.table(out)

```

